%\documentclass[10pt,a4paper]{article}

\documentclass[24pt]{article}

\usepackage{arxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
%\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}         % Can be removed after putting your text content
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amssymb}
\usepackage{amsthm}


\usepackage{tikz} 
\usepackage{caption}
\usepackage{amsmath}
\usepackage{cleveref}       % smart cross-referencing
\usepackage{colortbl}
\usepackage{color}
\usepackage{listings}
\usepackage{multicol}


\definecolor{orange151}{rgb}{0.9,0.647,0}
\definecolor{lgreen}{rgb}{0.564,0.93,0.564}


\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=none,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\definecolor{dgreen}{rgb}{0,0.5,0}
\definecolor{bg}{rgb}{0.125,0.51,0.49}
\definecolor{mag}{rgb}{0.866,0.627,0.866}
\definecolor{lgray}{rgb}{0.49,0.49,0.49}
\definecolor{dgray}{rgb}{0.82,0.788,0.827}
\definecolor{pink}{rgb}{1, 0.568, 0.686}
\definecolor{lblue}{rgb}{0.078, 0.741, 0.931}
\definecolor{orag2}{rgb}{0.87, 0.478, 0.12}

\newcommand*{\addheight}[2][.5ex]{%
  \raisebox{0pt}[\dimexpr\height+(#1)\relax]{#2}%
}

%\newcommand{\subf}[2]{%
%  {\small\begin{tabular}[t]{@{}c@{}}
%  #1\\#2
%  \end{tabular}}%
%}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{definition}[theorem]{Definition}


\title{Cheat sheet: pytorch and tensorflow}

%\author{ \href{https://orcid.org/0000-0002-8749-3324}{\includegraphics[scale=0.08]{orcid.pdf} \href{mailto: jacques.bourg739@gmail.com}{@}\hspace{1mm} Jacques Bourg    }}
\author{Jacques Bourg}


% Uncomment to override  the `A preprint' in the header
\renewcommand{\headeright}{}
\renewcommand{\undertitle}{}
\renewcommand{\shorttitle}{Cheat sheet: pytorch and tensorflow}


\hypersetup{
pdftitle={Machine learning for tabular data: methodology},
pdfsubject={math.NT},
pdfauthor={Jacques Bourg},
pdfkeywords={Deep learning frameworks, pytorch, Tensorflow},
}
 

\begin{document}

\maketitle

%\begin{abstract}
 
%\end{abstract}

\keywords{Deep learning frameworks, Pytorch, Tensorflow}

\section{Imports}

\begin{multicols}{2}
\begin{lstlisting}
import torch 
\end{lstlisting}

\begin{lstlisting}
import tensorflow as tf
\end{lstlisting}
\end{multicols}  
 
 
\section{GPU availability} 
 
\begin{multicols}{2}
\begin{lstlisting}
torch.cuda.is_available()
\end{lstlisting}
\columnbreak

\begin{lstlisting}
logical_gpus = tf.config.experimental.list_logical_devices('GPU')
if len(logical_gpus) > 0:
    print("GPU is available and can be used by TensorFlow")
    
memory_growth = tf.config.experimental.get_memory_growth()
for device, growth in zip(tf.config.list_physical_devices('GPU'), memory_growth):
    print(f"{device.name} memory growth: {growth}")    
    
\end{lstlisting}
\end{multicols}  
  
\section{Variables} 
 
\begin{multicols}{2}
\begin{lstlisting}
d0 = torch.ones(1)
d1 = torch.ones(2)
d2 = torch.ones(2, 2)
d3 = torch.ones(2, 2, 2)
\end{lstlisting}
\columnbreak

\begin{lstlisting}
d0 = tf.ones((1,))
d1 = tf.ones((2,))
d2 = tf.ones((2, 2))
d3 = tf.ones((2, 2, 2))
\end{lstlisting}
\end{multicols} 
 
 
 
\section{Conversion}


\begin{multicols}{2}
\begin{lstlisting}
d0_np = torch.ones(2).numpy()
d0_torch = torch.from_numpy(numpy_array)
\end{lstlisting}
\columnbreak

\begin{lstlisting}
d0_np = tf.ones((2,)).numpy()
d0_tf = tf.convert_to_tensor(numpy_array)

\end{lstlisting}
\end{multicols}



\section{Basic operations}


\begin{multicols}{2}
\begin{lstlisting}
result = tensor1 + tensor2
result = tensor1 * tensor2
result = torch.matmul(tensor1, tensor2)
x_t = x.t()
\end{lstlisting}
\columnbreak

\begin{lstlisting}
result = tf.add(tensor1, tensor2)
result = tf.multiply(tensor1, tensor2)
result = tf.matmul(tensor1, tensor2)
x_t = tf.transpose(x)

\end{lstlisting}
\end{multicols}



\section{Dimensionality}


\begin{multicols}{2}
\begin{lstlisting}
x.size()                                  
x = torch.cat([tensor1, tensor2], dim=0)          
y = x.view(a,b,...)                       
y = x.view(-1,a)                          
y = x.transpose(a,b)                      
y = x.permute(*dims)                      
y = x.unsqueeze(dim)                      
y = x.unsqueeze(dim=2)                    
y = x.squeeze()                           
y = x.squeeze(dim=1)
\end{lstlisting}
\columnbreak

\begin{lstlisting}
tf.shape(x)
x = tf.concat([tensor1, tensor2], axis=0)
y = tf.reshape(x, new_shape)
y = tf.reshape(x, (-1, a))
y = tf.transpose(x, perm)
y = tf.transpose(x, perm)
y = tf.expand_dims(x, axis)
y = tf.expand_dims(x, axis=2)
y = tf.squeeze(x)
y = tf.squeeze(x, axis=1)
\end{lstlisting}
\end{multicols}






\section{Automatic differentiation}
 
 
\begin{multicols}{2}
\begin{lstlisting}
x = torch.randn(2, 2, requires_grad=True)
y = x.pow(2).sum()
y.backward()
print(x.grad) 
\end{lstlisting}
\columnbreak

\begin{lstlisting}
with tf.GradientTape() as tape:
  y = x**2

grads = tape.gradient(y, x)
\end{lstlisting}
\end{multicols}
 

 
\section{Neural network layers and activation functions} 


\begin{multicols}{2}
\begin{lstlisting}
import torch.nn as nn
linear_layer = nn.Linear(input_size, output_size)
conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size)
nn.ReLU()
nn.Sigmoid()
nn.Tanh()
nn.MaxPoolXd(s)
nn.BatchNormXd

nn.RNN/LSTM/GRU
nn.Dropout(p=0.5, inplace=False)
nn.Dropout2d(p=0.5, inplace=False)
nn.Embedding(num_embeddings, embedding_dim)
\end{lstlisting}
\columnbreak

\begin{lstlisting}
import tensorflow.keras.layers as layers
linear_layer = layers.Dense(units=output_size)
conv_layer = layers.Conv2D(filters=out_channels, kernel_size=kernel_size)
tf.keras.layers.ReLU()
tf.keras.layers.Sigmoid()
tf.keras.layers.Tanh()
tf.keras.layers.MaxPoolXd(pool_size=s, padding='valid')
tf.keras.layers.BatchNormalization()
tf.keras.layers.SimpleRNN/LSTM/GRU
tf.keras.layers.Dropout(rate=0.5)
tf.keras.layers.SpatialDropout2D(rate=0.5)
tf.keras.layers.Embedding(input_dim=num_embeddings, output_dim=embedding_dim)
\end{lstlisting}
\end{multicols}

\newpage
\section{Networks}

\begin{multicols}{2}
\begin{lstlisting}
class Net(nn.Module):
    def __init__(self, input_size, output_size, hidden_sizes):
        super(Net, self).__init__()
        layers = [nn.Linear(input_size, hidden_sizes[0])]
        for i in range(1, len(hidden_sizes)):
            layers.append(nn.ReLU())  
            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))
        layers.append(nn.ReLU())  
        layers.append(nn.Linear(hidden_sizes[-1], output_size))
        self.sequential = nn.Sequential(*layers)

    def forward(self, x):
        return self.sequential(x)

net = Net(input_size=10, output_size=2, hidden_sizes=[5, 10])


\end{lstlisting}
\columnbreak

\begin{lstlisting}
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

net = Sequential()
net.add(Dense(units=5, input_dim=10, activation='relu'))  
net.add(Dense(units=10, activation='relu'))  
net.add(Dense(units=2))  

net.compile(optimizer='adam', loss='mse')

\end{lstlisting}
\end{multicols}

\section{Model and optimizer}


\begin{multicols}{2}
\begin{lstlisting}
import torch.optim as optim
model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, output_size))
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=1e-5) 
\end{lstlisting}
\columnbreak

\begin{lstlisting}
model = tf.keras.Sequential([
    tf.keras.layers.Dense(hidden_size, activation='relu', input_shape=(input_size,)),
    tf.keras.layers.Dense(output_size)])
model.compile(optimizer='adam', loss='mse')
\end{lstlisting}

\end{multicols}


\section{Train a network}

\begin{multicols}{2}
\begin{lstlisting}
for i, (inputs, labels) in enumerate(train_loader):   
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()   

        if i % 100 == 0:
            print(f"Epoch {epoch+1}, Batch {i+1}, Loss: {loss.item():.4f}"))\end{lstlisting}
\columnbreak

\begin{lstlisting}
for epoch in range(num_epochs):
    for inputs, labels in train_dataset:
        loss = model.train_on_batch(inputs, labels)
        if i % 100 == 0:
            print(f"Epoch {epoch+1}, Batch {i+1}, Loss: {loss:.4f}")
\end{lstlisting}
\end{multicols}  



\bibliographystyle{unsrtnat}
\bibliography{references}


\end{document}







